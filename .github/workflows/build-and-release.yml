name: Build llama.cpp with ROCm HIP (gfx1151)

on:
  push:
    tags:
      - "b*"
  workflow_dispatch:

permissions:
  contents: write

jobs:
  build:
    strategy:
      matrix:
        include:
          - ROCM_VER: '7.1.1'
            ROCM_INSTALLER_URL: https://repo.radeon.com/amdgpu-install/7.1.1/ubuntu/noble/amdgpu-install_7.1.1.70101-1_all.deb
          - ROCM_VER: '7.2.0'
            ROCM_INSTALLER_URL: https://repo.radeon.com/amdgpu-install/7.2/ubuntu/noble/amdgpu-install_7.2.70200-1_all.deb

    name: Build with HIP support
    runs-on: blacksmith-2vcpu-ubuntu-2404

    steps:
      # 1 Checkout this repo
      - name: Checkout source
        uses: actions/checkout@v6
    
      - name: Set version variables
        run: |
          echo "Setting LLAMA_CPP_VER variable"
          source ./llama_cpp_version
          export "ROCM_VER=${{ matrix.ROCM_VER }}"
          echo "LLAMA_CPP_VER=$LLAMA_CPP_VER" >> $GITHUB_ENV
          echo "ROCM_VER=${{ matrix.ROCM_VER }}" >> $GITHUB_ENV

      # 2 Install CMake + build tools
      - name: Install build tools
        run: |
          sudo add-apt-repository universe
          sudo apt update
          sudo apt-get install -y \
            cmake \
            build-essential \
            ninja-build \
            ccache \
            libcurl4-openssl-dev \
            gcc-14 g++-14

      # 3 Install ROCm dev packages for compile-time HIP support
      - name: Install ROCm / HIP development packages
        run: |
          wget ${{ matrix.ROCM_INSTALLER_URL }} -O amdgpu-install.deb
          sudo apt install -y ./amdgpu-install.deb
          sudo apt update
          sudo apt install -y python3-setuptools python3-wheel
          sudo usermod -a -G render,video $LOGNAME
          sudo apt install -y rocm
        

      - name: Verify HIP compiler
        run: |
          hipcc --version

      # 4 Clone the official llama.cpp repo
      - name: Clone llama.cpp
        run: |
          git clone --depth 1 --branch $LLAMA_CPP_VER https://github.com/ggml-org/llama.cpp.git

      # 5 Configure & build with HIP and gfx1151
      - name: Configure and Build llama.cpp
        run: |
          cd llama.cpp
          sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-14 100
          sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-14 100
          gcc --version
          g++ --version
          export CFLAGS="-O3 -march=znver5 -mtune=znver5 -flto=auto"
          export CXXFLAGS="$CFLAGS"
          cmake -B build \
            -DCMAKE_BUILD_TYPE=Release \
            -DGGML_HIP=ON \
            -DGGML_NATIVE=OFF \
            -DLLAMA_BUILD_TESTS=OFF \
            -DLLAMA_BUILD_EXAMPLES=OFF \
            -DAMDGPU_TARGETS="gfx1151"
          cmake --build build -- -j$(nproc)

      # 6 Package compiled binaries
      - name: Package artifacts
        run: |
          tar -cJf llama-cpp-${LLAMA_CPP_VER}-rocm-${ROCM_VER}-gfx1151.tar.xz -C llama.cpp/build/bin .

      # 7 Upload the built artifacts
      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: llama-cpp-strix-halo-${{ matrix.ROCM_VER }}
          path: llama-cpp-*.tar.xz
          retention-days: 1

  release:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref_type == 'tag'
    
    steps:
      - name: Checkout source
        uses: actions/checkout@v6
 
      - name: Set version variables
        run: |
          echo "Setting LLAMA_CPP_VER variable"
          source ./llama_cpp_version
          echo "LLAMA_CPP_VER=$LLAMA_CPP_VER" >> $GITHUB_ENV

      - name: Download llama-cpp-strix-halo artifact
        uses: actions/download-artifact@v7
        with:
          pattern: llama-cpp-strix-halo-*
          path: ./release
          merge-multiple: true
      
      - name: List files
        run: |
          echo "Files to be released:"
          ls -lh ./release/
      
      - name: Create GitHub Release
        uses: softprops/action-gh-release@v2
        with:
          files: ./release/*
          body: |
            `llama.cpp` (${{ env.LLAMA_CPP_VER }}) compiled for Linux x86_64 with ROCm, optimized for AMD Strix Halo devices.
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
